import configparser
import itertools
import pickle
import re
import tempfile
from concurrent.futures.thread import ThreadPoolExecutor
from dataclasses import dataclass
from datetime import datetime
from io import StringIO
from types import SimpleNamespace

import pytest
from _pytest._io.terminalwriter import TerminalWriter
from _pytest.config import Config, create_terminal_writer
from _pytest.reports import TestReport
from strip_ansi import strip_ansi

from pytest_tui.__main__ import Cli, tui_launch
from pytest_tui.html import main as tuihtml
from pytest_tui.utils import (CONFIGFILE, MARKED_TERMINAL_OUTPUT_FILE, MARKERS,
                              REPORT_OBJECTS_FILE,
                              TEST_TUI_RESULT_OBJECTS_FILE,
                              UNMARKED_TERMINAL_OUTPUT_FILE,
                              errors_section_matcher, failures_section_matcher,
                              lastline_matcher, passes_section_matcher,
                              rerun_summary_matcher,
                              short_test_summary_matcher,
                              short_test_summary_test_matcher,
                              test_session_starts_matcher,
                              test_session_starts_test_matcher,
                              warnings_summary_matcher)

# Don't collect tests from any of these files
collect_ignore = [
    "setup.py",
    "plugin.py",
]

# A list of TestReport objects generated by Pytest during test run;
# each TestReport represents a single test's operation during one of
# Pytest's three phases: setup | call | teardown
reports = []
tui_test_results = []
skips = []
skip_idx = -1

@dataclass
class TuiTestResult:
    fqtn: str = ""
    outcome: str = ""
    start_time: datetime = None
    duration: float = 0.0
    caplog: str = ""
    capstderr: str = ""
    capstdout: str = ""
    longreprtext: str = ""
    skipped_idx = None


def pytest_addoption(parser):
    """Define the plugin's option flags as presented by Pytest"""
    group = parser.getgroup("tui")
    group.addoption(
        "--tui",
        action="store_true",
        help="Enable the pytest-tui plugin. Both text user interface (TUI) and HTML output are supported.",
    )


def add_ansi_to_report(config: Config, report: TestReport):
    """
    If the report has longreprtext (traceback info), mark it up with ANSI codes
    From https://stackoverflow.com/questions/71846269/algorithm-for-extracting-first-and-last-lines-from-sectionalized-output-file
    """
    buf = StringIO()
    buf.isatty = lambda: True

    reporter = config.pluginmanager.getplugin("terminalreporter")
    original_writer = reporter._tw
    writer = create_terminal_writer(config, file=buf)
    reporter._tw = writer

    reporter._outrep_summary(report)
    buf.seek(0)
    ansi = buf.read()
    buf.close()

    report.ansi = SimpleNamespace()
    setattr(report.ansi, "val", ansi)

    reporter._tw = original_writer


def pytest_report_teststatus(report: TestReport, config: Config):
    """Construct list(s) of individual TestReport instances"""

    if hasattr(report, "caplog") and report.caplog:
        for tui_test_result in tui_test_results:
            if tui_test_result.fqtn == report.nodeid:
                tui_test_result.caplog = report.caplog

    if hasattr(report, "capstderr") and report.capstderr:
        for tui_test_result in tui_test_results:
            if tui_test_result.fqtn == report.nodeid:
                tui_test_result.capstderr = report.capstderr

    if hasattr(report, "capstdout") and report.capstdout:
        for tui_test_result in tui_test_results:
            if tui_test_result.fqtn == report.nodeid:
                tui_test_result.capstdout = report.capstdout

    if hasattr(report, "longreprtext") and report.longreprtext:
        add_ansi_to_report(config, report)
        for tui_test_result in tui_test_results:
            if tui_test_result.fqtn == report.nodeid:
                tui_test_result.longreprtext = report.ansi.val

    reports.append(report)


@pytest.hookimpl()
def pytest_runtest_logstart(nodeid, location):
    for tui_test_result in tui_test_results:
        if tui_test_result.fqtn == nodeid:
            tui_test_result.start_time = datetime.now().strftime("%m/%d/%Y %H:%M:%S.%f")
            break


@pytest.hookimpl(trylast=True)
def pytest_configure(config: Config) -> None:
    if not hasattr(config.option, "tui"):
        return
    if not config.option.tui:
        return

    config.option.verbose = 1  # easier parsing of final test results
    config.option.reportchars = "A"  # "display all" mode so all results are shown

    # Examine Pytest terminal output to mark different sections of the output.
    # This code is based on the code in pytest's `pastebin.py`.
    tr = config.pluginmanager.getplugin("terminalreporter")
    if tr is not None:
        config._pytui_unmarked_outputfile = tempfile.TemporaryFile("wb+")
        config._pytui_marked_outputfile = tempfile.TemporaryFile("wb+")
        oldwrite = tr._tw.write

        # identify and mark each results section
        def tee_write(s, **kwargs):
            if re.search(test_session_starts_matcher, s):
                config._pytui_current_section = "test_session_starts"
                config._pytui_marked_outputfile.write(
                    (MARKERS["pytest_tui_test_session_starts"] + "\n").encode("utf-8")
                )
            if re.search(errors_section_matcher, s):
                config._pytui_current_section = "errors"
                config._pytui_marked_outputfile.write(
                    (MARKERS["pytest_tui_errors_section"] + "\n").encode("utf-8")
                )
            if re.search(failures_section_matcher, s):
                config._pytui_current_section = "failures"
                config._pytui_marked_outputfile.write(
                    (MARKERS["pytest_tui_failures_section"] + "\n").encode("utf-8")
                )
            if re.search(warnings_summary_matcher, s):
                config._pytui_current_section = "warnings_summary"
                config._pytui_marked_outputfile.write(
                    (MARKERS["pytest_tui_warnings_summary"] + "\n").encode("utf-8")
                )
            if re.search(passes_section_matcher, s):
                config._pytui_current_section = "passes"
                config._pytui_marked_outputfile.write(
                    (MARKERS["pytest_tui_passes_section"] + "\n").encode("utf-8")
                )
            if re.search(rerun_summary_matcher, s):
                config._pytui_current_section = "rerun_summary"
                config._pytui_marked_outputfile.write(
                    (MARKERS["pytest_tui_rerun_summary"] + "\n").encode("utf-8")
                )
            if re.search(short_test_summary_matcher, s):
                config._pytui_current_section = "short_test_summary"
                config._pytui_marked_outputfile.write(
                    (MARKERS["pytest_tui_short_test_summary"] + "\n").encode("utf-8")
                )
            if re.search(lastline_matcher, s):
                config._pytui_current_section = "lastline"
                config._pytui_marked_outputfile.write(
                    (MARKERS["pytest_tui_last_line"] + "\n").encode("utf-8")
                )

            # If this is an actual test outcome line in the initial `=== test session starts ==='
            # section, populate the TuiTestResult's fully qualified test name field. Do not add
            # duplicates (as may be encountered with plugins such as pytest-rerunfailures).
            if config._pytui_current_section == "test_session_starts" and re.search(
                test_session_starts_test_matcher, s
            ):
                fqtn = re.search(test_session_starts_test_matcher, s)[1]
                if fqtn not in [t.fqtn for t in tui_test_results]:
                    tui_test_results.append(TuiTestResult(fqtn=fqtn))

            # If this is an actual test outcome line in the `=== short test summary info ===' section,
            # populate the TuiTestResult's outcome field.
            if config._pytui_current_section == "short_test_summary" and re.search(
                short_test_summary_test_matcher, strip_ansi(s)
            ):

                outcome = re.search(
                    short_test_summary_test_matcher, strip_ansi(s)
                ).groups()[0]
                fqtn = re.search(short_test_summary_test_matcher, strip_ansi(s)).groups()[1]
                print("")

                for tui_test_result in tui_test_results:
                    if tui_test_result.fqtn == fqtn:
                        tui_test_result.outcome = outcome
                        break

            #     # We treat Skipped tests differently here, as Pytest displays their format in this section
            #     # differently than in the `=== test session starts ===' section, truncating their fqtns and
            #     # appending a line number instead of specifying their test names. It appears that these tests
            #     # are reported in these sections in ascending order of line number, so we can use this as a
            #     # way to correlate them with the order in which they were run. Unless the use takes specific
            #     # steps to run them in a different order (not sure how they would do this), it should be fine.
            #     # if outcome == "SKIPPED":
            #     # # if False:
            #     #     subbed = re.sub(r"(:\d+:).*", "", strip_ansi(s), 0, re.MULTILINE)
            #     #     fqtn = re.search(short_test_summary_test_matcher, subbed).groups()[1]
            #     #     possibles = sorted([tui_test_result for tui_test_result in tui_test_results if fqtn in tui_test_result.fqtn and tui_test_result.outcome == ""], key=lambda tui_test_result: tui_test_result.start_time)
            #     #     if len(possibles) > 0:
            #     #         if len(skips) > 0:
            #     #             skips.append(possibles)

            #         # for tui_test_result in tui_test_results:
            #         #     if tui_test_result.fqtn == fqtn:
            #         #         tui_test_result.outcome = outcome
            #         #         break

            #     for tui_test_result in tui_test_results:
            #         if tui_test_result.fqtn == fqtn:
            #             tui_test_result.outcome = outcome
            #             break


            #     else:
            #         # fqtn = re.search(
            #         #     short_test_summary_test_matcher, strip_ansi(s)
            #         # ).groups()[1]
            #         for tui_test_result in tui_test_results:
            #             if fqtn == tui_test_result.fqtn:
            #                 tui_test_result.outcome = outcome
            #                 break
            #             else:
            #                 tui_test_result.outcome = "SKIPPED"

            # Write this line's origina pytest output text (plus markup) to console
            oldwrite(s, **kwargs)

            # Markup this line's text by passing it to an instance of TerminalWriter's
            # 'markup' method. Do not pass "flush" to the method, or it will throw an error.
            s1 = s
            kwargs.pop("flush") if "flush" in kwargs else None
            s1 = TerminalWriter().markup(s, **kwargs)

            # Encode the marked up line so it can be written to the config object.
            # The Pytest config object can be used by plugins for conveying staeful
            # info across an entire test run session.
            if isinstance(s1, str):
                marked_up = s1.encode("utf-8")
            config._pytui_marked_outputfile.write(marked_up)

            # Write this line's original (unmarked) text to unmarked file
            s_orig = s
            kwargs.pop("flush") if "flush" in kwargs else None
            s_orig = TerminalWriter().markup(s, **kwargs)
            if isinstance(s_orig, str):
                unmarked_up = s_orig.encode("utf-8")
            config._pytui_unmarked_outputfile.write(unmarked_up)

        # Write to both terminal/console and tempfiles:
        # _pytui_marked_outputfile, _pytui_unmarked_outputfile
        tr._tw.write = tee_write


def pytest_unconfigure(config: Config):
    # Populate test result objects with total durations, from each test's
    # TestReport objeects.
    for tui_test_result, test_report in itertools.product(tui_test_results, reports):
        if test_report.nodeid == tui_test_result.fqtn:
            tui_test_result.duration += test_report.duration

    if hasattr(config, "_pytui_marked_outputfile"):
        config._pytui_marked_outputfile.seek(0)
        markedsessionlog = config._pytui_marked_outputfile.read()
        config._pytui_marked_outputfile.close()

    if hasattr(config, "_pytui_unmarked_outputfile"):
        config._pytui_unmarked_outputfile.seek(0)
        unmarkedsessionlog = config._pytui_unmarked_outputfile.read()
        config._pytui_unmarked_outputfile.close()
        config.pluginmanager.getplugin("terminalreporter")
        with open(MARKED_TERMINAL_OUTPUT_FILE, "wb") as marked_file:
            marked_file.write(markedsessionlog)
        with open(UNMARKED_TERMINAL_OUTPUT_FILE, "wb") as unmarked_file:
            unmarked_file.write(unmarkedsessionlog)
        # with open(REPORT_OBJECTS_FILE, "wb") as report_file:
        #     pickle.dump(reports, report_file)
        with open(TEST_TUI_RESULT_OBJECTS_FILE, "wb") as test_result_file:
            pickle.dump(tui_test_results, test_result_file)

    if hasattr(config.option, "tui") and config.option.tui:
        pytui_tui(config)


def pytui_tui(config: Config) -> None:
    """
    Final code invocation after Pytest run has completed.
    Will call either or both of TUI / HTML code is specified on cmd line.
    """
    config_parser = configparser.ConfigParser()

    # Make sure the config file exists and has section content
    try:
        config_parser.read(CONFIGFILE)
        assert len(config_parser.sections()) > 0
    except Exception:
        Cli().apply_default_config()
    finally:
        config_parser.read(CONFIGFILE)

    try:
        # disable capturing while TUI runs to avoid error `redirected stdin is pseudofile, has
        # no fileno()`; adapted from https://githubmemory.com/repo/jsbueno/terminedia/issues/25
        capmanager = config.pluginmanager.getplugin("capturemanager")
        capmanager.suspend_global_capture(in_=True)
    finally:
        with ThreadPoolExecutor() as executor:
            executor.submit(tuihtml)
        if config_parser["TUI"].get("autolaunch_tui") == "True":
            tui_launch()

        capmanager.resume_global_capture()
